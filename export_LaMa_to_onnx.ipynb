{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Export LaMa inpainting model to ONNX\n",
    "\n",
    "\n",
    "Author: [Nikita Selin](https://github.com/OPHoperHPO), [Carve.Photos Team](https://carve.photos) \\\n",
    "HuggingFace Repository with ONNX Model: [Link](https://huggingface.co/Carve/LaMa-ONNX) \\\n",
    "Original repository: [Link](https://github.com/advimman/lama)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "2I-ey_v2V84-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## install deps"
   ],
   "metadata": {
    "id": "ZpWNdm0easCW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uD8BlL29U4mx",
    "outputId": "bad7e74a-aa9f-4788-d2da-10e91eda21fb"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Carve-Photos/lama --depth 1"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd lama"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bIOciP0QXN4z",
    "outputId": "8fc081e0-f739-478d-d5f8-a63125729b0c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!curl -LJO https://huggingface.co/smartywu/big-lama/resolve/main/big-lama.zip\n",
    "!unzip big-lama.zip"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "meujpEotXS8W",
    "outputId": "49c58607-a409-43d3-db7b-ae560e2c2796"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip3 install omegaconf webdataset pytorch_lightning pytorch_lightning kornia==0.5.0 onnx onnxruntime"
   ],
   "metadata": {
    "id": "JCM7I-zYXj6G"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## init model"
   ],
   "metadata": {
    "id": "_pqiG3G4bFHU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from yaml import safe_load\n",
    "\n",
    "from saicinpainting.training.trainers.default import (\n",
    "    DefaultInpaintingTrainingModule,\n",
    ")\n",
    "\n",
    "\n",
    "class ExportLama(torch.nn.Module):\n",
    "    def forward(self, image: torch.Tensor, mask: torch.Tensor):\n",
    "        masked_img = image * (1 - mask)\n",
    "\n",
    "        if self.model.concat_mask:\n",
    "            masked_img = torch.cat([masked_img, mask], dim=1)\n",
    "\n",
    "        predicted_image = self.model.generator(masked_img)\n",
    "        inpainted = mask * predicted_image + (1 - mask) * image\n",
    "        return torch.clamp(inpainted * 255, min=0, max=255)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6U8hoe8Oaamy",
    "outputId": "5972d0b8-c660-47ac-c6cc-1ad6f72a378d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## export onnx"
   ],
   "metadata": {
    "id": "Uw3GqXdhbIUU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the configuration file\n",
    "config = OmegaConf.create(safe_load(open(\"/content/lama/big-lama/config.yaml\")))\n",
    "\n",
    "# Extract and modify training model configuration\n",
    "kwargs = dict(config.training_model)\n",
    "kwargs.pop(\"kind\")\n",
    "kwargs[\"use_ddp\"] = True\n",
    "\n",
    "# Enable JIT version of FourierUnit, required for export\n",
    "config.generator.resnet_conv_kwargs.use_jit = True\n",
    "\n",
    "# Fix the configuration by setting the weight to zero\n",
    "config.losses.resnet_pl.weight = 0\n",
    "\n",
    "# Load the model state\n",
    "state = torch.load(\"/content/lama/big-lama/models/best.ckpt\", map_location=\"cpu\")\n",
    "lama_dilated_model = DefaultInpaintingTrainingModule(config, **kwargs)\n",
    "lama_dilated_model.load_state_dict(state[\"state_dict\"], strict=False)\n",
    "lama_dilated_model.on_load_checkpoint(state)\n",
    "lama_dilated_model.freeze()\n",
    "lama_dilated_model.eval()\n",
    "\n",
    "# Export the model\n",
    "exported_model = ExportLama()\n",
    "exported_model.register_module(\"model\", lama_dilated_model)\n",
    "exported_model.eval()\n",
    "exported_model.to(\"cpu\")\n",
    "\n",
    "# Export to ONNX format\n",
    "torch.onnx.export(\n",
    "    exported_model,\n",
    "    (\n",
    "        torch.rand(1, 3, 512, 512).type(torch.float32).to(\"cpu\"),  # Change resolution here! If you get a tensor size mismatch, you need to specify correct padding (see FourierUnitJIT)\n",
    "        torch.rand(1, 1, 512, 512).type(torch.float32).to(\"cpu\")\n",
    "    ),\n",
    "    \"/content/lama_fp32.onnx\",\n",
    "    input_names=[\"image\", \"mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"image\": {0: \"batch\"},\n",
    "        \"mask\": {0: \"batch\"},\n",
    "        \"output\": {0: \"batch\"}\n",
    "    },  # TODO: Adapt FourierUnit to support dynamic axes (see irfttn and rfft for correct padding)\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    opset_version=17,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"Lama Model exported to /content/lama_fp32.onnx (open file explorer to download)\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5g0q1YhqalQY",
    "outputId": "0f2cb12d-3484-4639-c807-c0acfddf71d6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test exported onnx model"
   ],
   "metadata": {
    "id": "6TmS901NfBNs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import torch\n",
    "import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "def get_image(image):\n",
    "    if isinstance(image, Image.Image):\n",
    "        img = np.array(image)\n",
    "    elif isinstance(image, np.ndarray):\n",
    "        img = image.copy()\n",
    "    else:\n",
    "        raise Exception(\"Input image should be either PIL Image or numpy array!\")\n",
    "\n",
    "    if img.ndim == 3:\n",
    "        img = np.transpose(img, (2, 0, 1))  # chw\n",
    "    elif img.ndim == 2:\n",
    "        img = img[np.newaxis, ...]\n",
    "\n",
    "    assert img.ndim == 3\n",
    "\n",
    "    img = img.astype(np.float32) / 255\n",
    "    return img\n",
    "\n",
    "\n",
    "def ceil_modulo(x, mod):\n",
    "    if x % mod == 0:\n",
    "        return x\n",
    "    return (x // mod + 1) * mod\n",
    "\n",
    "\n",
    "def scale_image(img, factor, interpolation=cv2.INTER_AREA):\n",
    "    if img.shape[0] == 1:\n",
    "        img = img[0]\n",
    "    else:\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "    img = cv2.resize(img, dsize=None, fx=factor, fy=factor, interpolation=interpolation)\n",
    "\n",
    "    if img.ndim == 2:\n",
    "        img = img[None, ...]\n",
    "    else:\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "    return img\n",
    "\n",
    "\n",
    "def pad_img_to_modulo(img, mod):\n",
    "    channels, height, width = img.shape\n",
    "    out_height = ceil_modulo(height, mod)\n",
    "    out_width = ceil_modulo(width, mod)\n",
    "    return np.pad(\n",
    "        img,\n",
    "        ((0, 0), (0, out_height - height), (0, out_width - width)),\n",
    "        mode=\"symmetric\",\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_img_and_mask(image, mask, device, pad_out_to_modulo=8, scale_factor=None):\n",
    "    out_image = get_image(image)\n",
    "    out_mask = get_image(mask)\n",
    "\n",
    "    if scale_factor is not None:\n",
    "        out_image = scale_image(out_image, scale_factor)\n",
    "        out_mask = scale_image(out_mask, scale_factor, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    if pad_out_to_modulo is not None and pad_out_to_modulo > 1:\n",
    "        out_image = pad_img_to_modulo(out_image, pad_out_to_modulo)\n",
    "        out_mask = pad_img_to_modulo(out_mask, pad_out_to_modulo)\n",
    "\n",
    "    out_image = torch.from_numpy(out_image).unsqueeze(0).to(device)\n",
    "    out_mask = torch.from_numpy(out_mask).unsqueeze(0).to(device)\n",
    "\n",
    "    out_mask = (out_mask > 0) * 1\n",
    "\n",
    "    return out_image, out_mask\n",
    "\n",
    "def open_image(image):\n",
    "    if isinstance(image, str):\n",
    "      if image.startswith(\"http://\") or image.startswith(\"https://\"):\n",
    "        image = Image.open(io.BytesIO(requests.get(image).content))\n",
    "      else:\n",
    "        image = Image.open(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "z-tBP8qgeKVC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sess_options = onnxruntime.SessionOptions()\n",
    "model = onnxruntime.InferenceSession('/content/lama_fp32.onnx', sess_options=sess_options)\n"
   ],
   "metadata": {
    "id": "fwUVncL6enJ6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title predict with onnx model\n",
    "image_url = \"https://huggingface.co/Carve/LaMa-ONNX/resolve/main/image.jpg\" # @param {type:\"string\"}\n",
    "mask_url = \"https://huggingface.co/Carve/LaMa-ONNX/resolve/main/mask.png\" # @param {type:\"string\"}\n",
    "\n",
    "image = open_image(image_url).resize((512, 512))\n",
    "mask = open_image(mask_url).convert(\"L\").resize((512, 512))\n",
    "\n",
    "image, mask = prepare_img_and_mask(image, mask, 'cpu')\n",
    "# Run the model\n",
    "outputs = model.run(None,\n",
    "                    {'image': image.numpy().astype(np.float32),\n",
    "                     'mask': mask.numpy().astype(np.float32)})\n",
    "\n",
    "output = outputs[0][0]\n",
    "# Postprocess the outputs\n",
    "output = output.transpose(1, 2, 0)\n",
    "output = output.astype(np.uint8)\n",
    "output = Image.fromarray(output)\n",
    "output"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "AEcIuUpleXuZ",
    "outputId": "21f27229-c3c9-45e4-df01-36a511aef7b9"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}